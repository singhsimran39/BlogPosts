{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the Neural Network is:\n",
    "- Input layer with 784 neurons\n",
    "- Hidden layer with 100 neurons\n",
    "- Output layer with 10 neurons (number of classes)\n",
    "\n",
    "Just as the output layer the hidden layer can be considered as a layer with 100 classes. The input to each neuron in the hidden layer is the weighted sum of each feature of a training example. To this input the hidden layer applies the ReLU function. This output goes as input to the output layer of the network where each neuron again receives the input as the weighted sum of ReLU values and second layer weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one training example the dimensions of the network are:\n",
    "- Input layer: $X^{(1)}$ => 784 $\\times$ 1\n",
    "- First layer weights: $W^{(1)}$ => 100 $\\times$ 784 \n",
    "- Hidden layer: $Z^{(2)} = W^{(1)} X^{(1)}$ => 100 $\\times$ 1\n",
    "- Hidden layer activated: $a^{(2)} = f(Z^{(2)})$ => 100 $\\times$ 1, where $f(x) = max(0, x)$ i.e. ReLU\n",
    "- Second/Output layer weights: $W^{(2)}$ => 10 $\\times$ 100\n",
    "- Output layer: $Z^{(3)} = W^{(2)} a^{(2)}$ => 10 $\\times$ 1\n",
    "- Output: Softmax classifier, \n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\frac{\\mathrm{e}^{Z^{(3)}}}{\\sum_{C}\\mathrm{e}^{Z^{(3)}}}\n",
    "\\end{equation}\n",
    "\n",
    "where C is the number of classes (10 in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network uses the Cross Entropy Loss Function which is of the form:\n",
    "\\begin{equation}\n",
    "    E = - \\sum\\limits_{j=1}^{C}t_{j}\\log{\\hat{y}_{j}}\n",
    "\\end{equation}\n",
    "\n",
    "For backpropagation we need to calculate the derivative of E w.r.t. $W^{(2)}$ and $W^{(1)}$.\n",
    "Using the chain rule we get (for one training example):\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{W^{(2)}}} = - \\frac{\\partial{E}}{\\partial{\\hat{y}_{j}}}\n",
    "    \\frac{\\partial\\hat{y}_{j}}{\\partial{Z^{(3)}}}\n",
    "    \\frac{\\partial{Z^{(3)}}}{\\partial{W^{(2)}}}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{W^{(1)}}} = - \\frac{\\partial{E}}{\\partial{\\hat{y}_{j}}} \n",
    "    \\frac{\\partial{\\hat{y}_{j}}}{\\partial{Z^{(3)}}}\n",
    "    \\frac{\\partial{Z^{(3)}}}{\\partial{a^{(2)}}}\n",
    "    \\frac{\\partial{a^{(2)}}}{\\partial{Z^{(2)}}}\n",
    "    \\frac{\\partial{Z^{(2)}}}{\\partial{W^{(1)}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{Z^{(3)}}}{\\partial{W^{(2)}}} = a^{(2)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{Z^{(3)}}}{\\partial{a^{(2)}}} = W^{(2)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{a^{(2)}}}{\\partial{Z^{(2)}}} = f'(Z^{(2)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{Z^{(2)}}}{\\partial{W^{(1)}}} = Z^{(1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While calculating the above 4 derivatives is pretty simple but finding the values of $\\frac{\\partial{E}}{\\partial{\\hat{y}_{i}}}$ and $\\frac{\\partial{\\hat{y}}_{j}}{\\partial{Z}^{(3)}}$ is more tricky. The value of the product $\\frac{\\partial{E}}{\\partial{\\hat{y}_{i}}} \\frac{\\partial{\\hat{y}}_{j}}{\\partial{Z}^{(3)}}$ is ($\\hat{y} - t$) where $\\hat{y}$ is the predicted class probability vector and $t$ is the actual one hot encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above information we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{W}^{(2)}} = - \\ (\\hat{y} - t)\\ a^{(2)}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{W}^{(1)}} = - \\ (\\hat{y} - t)\\  W^{(2)} \\ f'(Z^{(2)}) \\ Z^{(1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another inportant thing to note here is that $f(Z^{(2)})$ is the ReLU activation unit and its derivative $f'(Z^{(2)})$ is just a matirx of zeros and ones. So for the derivative the hidden layer matrix ($100 \\times 1$) will have ones where the value is greater than 0 and zeros elsewhere.\n",
    "\n",
    "The below code implements the above neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mnist_to_df(file_name, is_label):\n",
    "    with open(file_name, 'rb') as mnist_file:\n",
    "        if is_label:\n",
    "            magic_nr, size = struct.unpack(\">II\", mnist_file.read(8))\n",
    "            data = np.fromfile(mnist_file, dtype=np.uint8)\n",
    "            pd_data = pd.DataFrame(data, columns = ['digit'])\n",
    "        else:\n",
    "            magic_nr, size, rows, cols = struct.unpack(\">IIII\", mnist_file.read(16))\n",
    "            data = np.fromfile(mnist_file, dtype=np.uint8).reshape((size, rows*cols)).astype(np.float32)/256.\n",
    "            pd_data = pd.DataFrame(data, columns = [str(i) for i in range(rows*cols)])\n",
    "    return pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pixels = read_mnist_to_df('train-images-idx3-ubyte', False)\n",
    "train_labels = read_mnist_to_df('train-labels-idx1-ubyte', True)\n",
    "test_pixels = read_mnist_to_df('t10k-images-idx3-ubyte', False)\n",
    "test_labels = read_mnist_to_df('t10k-labels-idx1-ubyte', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization for hidden layer with 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = train_pixels.shape[1]\n",
    "hidden_neurons = 100\n",
    "num_classes = len(set(train_labels.digit))\n",
    "learn_rate = 0.01\n",
    "\n",
    "W1 = np.random.randn(hidden_neurons, input_layer) * ((2/input_layer)**(1/2))\n",
    "b1 = np.zeros((hidden_neurons, 1))\n",
    "W2 = np.random.randn(num_classes, hidden_neurons)\n",
    "b2 = np.zeros((num_classes, 1))\n",
    "\n",
    "xtrain = train_pixels.as_matrix()\n",
    "xtrain -= np.mean(train_pixels, axis=0)\n",
    "ytrain = np.zeros((train_labels.shape[0], 10))\n",
    "for i in range(len(ytrain)):\n",
    "    ytrain[i, train_labels.digit.iloc[i]] = 1\n",
    "\n",
    "ytrain = ytrain.astype('int')\n",
    "    \n",
    "xtest = test_pixels.as_matrix()\n",
    "xtest -= np.mean(test_pixels, axis=0)\n",
    "ytest = np.zeros((test_labels.shape[0], 10))\n",
    "for i in range(len(ytest)):\n",
    "    ytest[i, test_labels.digit.iloc[i]] = 1\n",
    "\n",
    "ytest = ytest.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network and predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochTrainLoss = []\n",
    "epochTestLoss = []\n",
    "trainActual = train_labels.digit.values\n",
    "testActual = test_labels.digit.values\n",
    "regLambda = .01\n",
    "\n",
    "for j in range(0, 80):\n",
    "\n",
    "    totalTrainLoss = 0\n",
    "    delta2 = np.zeros((num_classes, hidden_neurons))\n",
    "    delta1 = np.zeros((hidden_neurons, input_layer))\n",
    "\n",
    "    for i in range(0, xtrain.shape[0]):\n",
    "        #Forward Pass\n",
    "        X = xtrain[i:i+1, :]\n",
    "        hidden_layer = np.maximum(0, np.add(np.dot(W1, X.T), b1))\n",
    "        output_layer = np.add(np.dot(W2, hidden_layer), b2)\n",
    "        pred = np.exp(output_layer) / np.sum(np.exp(output_layer), axis=0)\n",
    "\n",
    "        #Loss\n",
    "        Y = ytrain[i:i+1, :]\n",
    "        trainLoss = -1 * np.sum(np.multiply(Y.T, np.log(pred)))\n",
    "\n",
    "        #Backward Pass\n",
    "        dEdW2 = -1 * np.dot((pred - Y.T), hidden_layer.T)\n",
    "        temp1 = -1 * np.dot(W2.T, (pred - Y.T))\n",
    "        temp1[temp1 <= 0] = 0\n",
    "        dEdW1 = np.dot(temp1, X)\n",
    "\n",
    "        totalTrainLoss += trainLoss\n",
    "        delta2 += dEdW2 + (regLambda * W2)\n",
    "        delta1 += dEdW1 + (regLambda * W1)\n",
    "\n",
    "    #Weight update\n",
    "    W2 += learn_rate * (delta2 / xtrain.shape[0])\n",
    "    W1 += learn_rate * (delta1 / xtrain.shape[0])\n",
    "    \n",
    "    hlTrain = np.maximum(0, np.add(np.dot(W1, xtrain.T), b1))\n",
    "    olTrain = np.add(np.dot(W2, hlTrain), b2)\n",
    "    finalTrain = np.exp(olTrain) / np.sum(np.exp(olTrain), axis=0)\n",
    "    trainPreds = np.argmax(finalTrain, axis=0)\n",
    "    trainAcc = np.sum(trainPreds == trainActual)\n",
    "    trainAcc = (trainAcc / xtrain.shape[0]) * 100\n",
    "    \n",
    "    hlTest = np.maximum(0, np.add(np.dot(W1, xtest.T), b1))\n",
    "    olTest = np.add(np.dot(W2, hlTest), b2)\n",
    "    finalTest = np.exp(olTest) / np.sum(np.exp(olTest), axis=0)\n",
    "    testPreds = np.argmax(finalTest, axis=0)\n",
    "    testAcc = np.sum(testPreds == testActual)\n",
    "    testAcc = (testAcc / xtest.shape[0]) * 100\n",
    "    \n",
    "    totalTestLoss = (-1 * np.sum(np.sum(np.multiply(ytest.T, np.log(finalTest)), axis=0))) / xtest.shape[0]\n",
    "    epochTestLoss.append(totalTestLoss)\n",
    "    epochTrainLoss.append(totalTrainLoss)\n",
    "    print('Epoch -', (j+1), ':')\n",
    "    print('Train Loss:', (totalTrainLoss / xtrain.shape[0]), ', Train Accuracy:', trainAcc, \n",
    "          ', Test Loss:', totalTestLoss, ', Test Accuracy:', testAcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
