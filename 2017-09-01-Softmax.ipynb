{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Function\n",
    "\n",
    "The Softmax function is a generalization of the Logistic Function. Where the Logistic function is mostly used in binary classification problems the Softmax function is used for multi class classification problems. The Softmax Function squashes a vector *Z* $\\in{R^{n}}$ to produce values in the range [0, 1]. The output of the softmax is normalized and the values can be interpreted as class probabilities. The form of the softmax is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\hat{y_{i}} = \\frac{\\mathrm{e}^{z_{i}}}{\\sum\\limits_{j=1}^{C}   \\mathrm{e}^{z_j}}\n",
    "\\end{equation}\n",
    "\n",
    "Where $C$ is the number of different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of Softmax Function\n",
    "If the softmax function is used in a neural network then its derivative is used for backpropagating the errors to the hidden layers. The derivative is calculated in 2 different cases when $i=j$ and when $i\\ne{j}$. The softmax can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{y_{i}} = \\frac{\\mathrm{e}^{z_{i}}} {\\mathrm{e}^{z_{1}} + \\mathrm{e}^{z_{2}} +...+ \\mathrm{e}^{z_{i}} + ... +  \\mathrm{e}^{z_{C}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $i=j$ (using the Quotient Rule), differentiating and rearranging the sofxtmax equation gives us:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{\\hat{y_{i}}}}{\\partial{z_{j}}} = \\frac{\\mathrm{e}^{z_{i}}}{\\sum_{C}} \n",
    "    \\frac{\\sum_{C} -\\mathrm{e}^{z_{j}}}{\\sum_{C}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{\\hat{y_{i}}}}{\\partial{z_{j}}} = \\hat{y_{i}} (1 - \\hat{y}_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $i \\ne{j}$ (using the Quotient Rule):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{\\hat{y_{i}}}}{\\partial{z_{j}}} = \\frac{0 - \\mathrm{e^{z_{i}}}\\mathrm{e^{z_{j}}}} {\\sum_{C}^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{\\hat{y_{i}}}}{\\partial{z_{j}}} = - \\hat{y_{i}} \\hat{y_{j}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss Function\n",
    "\n",
    "The softmax classifier generally uses the Cross Entropy Loss function which is of the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    E = - \\sum\\limits_{j=1}^{C} t_{j}\\log{\\hat{y_{j}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the output vector for the multi class classification is one hot encoded the $t_{j}$ is 1 for only the actual class that the example belongs to and zero at all other places. This reduces the form of the Cross Entropy Loss function to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    E = - t_{j} \\log{\\hat{y_{j}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $t_{j}$ and $\\hat{y}_{j}$ are 1 and predicted probability by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagating the Loss to the last hidden layer of the neural network\n",
    "\n",
    "The $\\hat{y}_{j}$ in the loss function comes from the softmax equation, which is a function of the $z$ therefore to backpropagate the error we must differentiate $E$ w.r.t. the values coming into the putput latyer and multiply that derivative with the derivative of the output which is 1. Using the chain rule to backpropagate the error we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = - \\sum\\limits_{j=1}^{C} \\frac{\\partial{E}}{\\partial{\\hat{y_{j}}}} \\frac{\\partial{\\hat{y_{j}}}}{\\partial{z_{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = - \\sum\\limits_{j=1}^{C} \\frac{t_{j}}{\\hat{y_{j}}} \\frac{\\partial{\\hat{y_{j}}}}{\\partial{z_{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have calculated the second part ($\\frac{\\partial{\\hat{y_{j}}}}{\\partial{z}_{i}}$) of the above equation in the previous section, thus substituting the values for when $i = j$ and $i\\ne{j}$ we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = - \\frac{t_{j}}{\\hat{y_{j}}} \\hat{y_{i}}(1 - \\hat{y_{i}}) - \\sum\\limits_{j=1, j\\ne{i}}^{C} \\frac{t_{j}}{\\hat{y_{j}}} (-\\hat{y_{j}}\\hat{y_{i}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = - t_{j}(1 - \\hat{y_{i}}) + \\sum\\limits_{j\\ne{i}} t_{j}\\hat{y_{i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = - t_{j} + t_{j}\\hat{y_{i}} + \\sum\\limits_{j\\ne{i}} t_{j}\\hat{y_{i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = - t_{j} + \\sum\\limits_{j=1}^{C} t_{j}\\hat{y_{i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since $\\hat{y_{i}}$ is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = - t_{j} + \\hat{y_{i}} \\sum\\limits_{j=1}^{C} t_{j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since for one hot encoding the sum of the vector t is equal to, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial{E}}{\\partial{z_{i}}} = \\hat{y_{i}} - t_{j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result can then be multiplied with the derivative of the hidden layer and backpropagated further down in the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
